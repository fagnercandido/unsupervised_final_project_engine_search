{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ed8fd0-96d6-4906-858f-1006b3a87e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fagnercandido/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/fagnercandido/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import ssl\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe8beb4-361c-4555-af2a-eed87d1cbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash import html\n",
    "from dash import dcc\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import Dash, html, dcc\n",
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4329004-9279-48de-95a8-694ea92d0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eligible_files():\n",
    "    elegible_files = []\n",
    "    for file in os.listdir('reuters21578'):\n",
    "        if file.endswith('.sgm'):\n",
    "            elegible_files.append(file)\n",
    "    return elegible_files\n",
    "\n",
    "def get_documents():\n",
    "    documents = []\n",
    "    files = get_eligible_files()\n",
    "    for file in files:\n",
    "        content = ''\n",
    "        try:\n",
    "            content = open(f'reuters21578/{file}', 'r', encoding=\"utf-8\").read()\n",
    "        except UnicodeDecodeError as e:\n",
    "            lines = []\n",
    "            for line in open(f'reuters21578/{file}', 'rb').readlines():\n",
    "                line = line.decode('utf-8','ignore') #.encode(\"utf-8\")\n",
    "                lines.append(line)\n",
    "            content = '\\n'.join(lines)\n",
    "        soup_document = BeautifulSoup(content.lower(), 'html.parser')\n",
    "        for document in soup_document.findAll('reuters'):\n",
    "            documents.append(document)\n",
    "    return documents\n",
    "\n",
    "def get_documents_by_dictionary_article():\n",
    "    articles = {}\n",
    "    for article in get_documents():\n",
    "        new_id = article.get('newid')\n",
    "        date_article = find_element_by_name(article, 'date')\n",
    "        topics = find_element_by_name(article, 'topics')\n",
    "        places = find_element_by_name(article, 'places')\n",
    "        people = find_element_by_name(article, 'people')\n",
    "        title = find_element_by_name(article, 'title')\n",
    "        dateline = find_element_by_name(article, 'dateline')\n",
    "        body = find_element_by_name(article, 'body')\n",
    "        metadados_article = {}\n",
    "        for item in ['date_article', 'dateline', 'topics', 'places', 'people', 'title', 'body']:\n",
    "            metadados_article[item] = eval(item)\n",
    "        articles[new_id] = metadados_article\n",
    "    return articles\n",
    "\n",
    "def find_element_by_name(article, tag):\n",
    "    element = article.find_all(tag)\n",
    "    if element:\n",
    "        return next((x.text for x in element))\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def preprocess_tokenize_and_lemmatize_and_add_frequency_matrix():\n",
    "    articles = get_documents_by_dictionary_article()\n",
    "    for item, value in articles.items():\n",
    "        articles[item]['body_lemmatized'] = remove_stop_word_and_lemmatize(value['body'])\n",
    "        articles[item]['topics_lemmatized'] = remove_stop_word_and_lemmatize(value['topics'], True) \n",
    "        articles[item]['body_frequency'] = create_frequency_matrix(articles[item]['body_lemmatized'])\n",
    "        articles[item]['body_tfidf'] = create_tf_idf(value['body'])\n",
    "    return articles\n",
    "\n",
    "def create_tf_idf(value):\n",
    "    if value:\n",
    "        cv = CountVectorizer()\n",
    "        word_count_vector = cv.fit_transform(list(filter(None, value.split('\\n'))))\n",
    "        \n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "        \n",
    "        count_vector = cv.transform(list(filter(None, value.split('\\n')))) \n",
    "        tf_idf_vector = tfidf_transformer.transform(count_vector)\n",
    "        \n",
    "        feature_names = cv.get_feature_names()\n",
    "        first_document_vector=tf_idf_vector[0]\n",
    "        dataframe = pd.DataFrame(first_document_vector.T.todense(), index = feature_names, columns=[\"tfidf\"])\n",
    "        dataframe = dataframe.sort_values(by = [\"tfidf\"], ascending=False)\n",
    "        \n",
    "        dataframe.dropna(inplace = True)\n",
    "        dataframe_to_dict = dataframe.to_dict()\n",
    "        return list(dataframe_to_dict.values())[0]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def remove_stop_word_and_lemmatize(value, is_topic = False):\n",
    "    result = []\n",
    "    if value:\n",
    "        for token in gensim.utils.simple_preprocess(value):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(nltk.stem.WordNetLemmatizer().lemmatize(token, pos='v'))\n",
    "    return set(result) if is_topic else result\n",
    "\n",
    "def create_frequency_matrix(value):\n",
    "    frequency_table = {}\n",
    "    for word in value:\n",
    "        if word in frequency_table:\n",
    "            frequency_table[word] += 1\n",
    "        else:\n",
    "            frequency_table[word] = 1\n",
    "    return frequency_table\n",
    "\n",
    "def search(query_query, articles):\n",
    "    articles_selected = {}\n",
    "    query_string_lemmatized = remove_stop_word_and_lemmatize(query_string)\n",
    "    for token in query_string_lemmatized:\n",
    "        for key, value in articles.items():\n",
    "            list_of_values = list(value['body_frequency'].keys())\n",
    "            if token in list_of_values and list_of_values.index(token) > 0.0:\n",
    "                if key in articles_selected:\n",
    "                    articles_selected[key]['relative_weight']  = articles_selected[key]['relative_weight'] + list_of_values.index(token)\n",
    "                else:\n",
    "                    articles_selected[key] = {'article': value, 'relative_weight': list_of_values.index(token)}\n",
    "    prepare_visualization(sorted(articles_selected.items(), key=lambda x: x[1]['relative_weight'], reverse=True))\n",
    "\n",
    "def prepare_visualization(articles):\n",
    "    print(f'\\t\\tMy Favorite Engine')\n",
    "    print()\n",
    "    for item in articles:\n",
    "        value = item[1]\n",
    "        print(f'{value[\"article\"][\"title\"]} - {value[\"article\"][\"date_article\"]}')\n",
    "        print(f'{value[\"article\"][\"body\"][:50]}...')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2548bab0-bd06-43a6-a2c1-42edfa5eb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = preprocess_tokenize_and_lemmatize_and_add_frequency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6911c3-9906-4cf8-a226-f443216c005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'music'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8987f32-b87f-444c-8ae1-aa01efd5a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMy Favorite Engine\n",
      "\n",
      "digital audio tape players go on sale in japan -  2-mar-1987 07:20:13.63\n",
      "japanese consumers hesitated about buying\n",
      "the cont...\n",
      "\n",
      "carolco <crc> may bid for lieberman <lman.o> -  2-jun-1987 08:32:00.28\n",
      "lieberman enterprises inc said\n",
      "\n",
      "carolco pictures i...\n",
      "\n",
      "international <icc.o> sets new sound technology -  2-jun-1987 14:28:36.79\n",
      "international cablecasting technologies\n",
      "inc, said ...\n",
      "\n",
      "electrosound <esg> says official has resigned -  2-jun-1987 13:51:03.09\n",
      "electrosound group inc said\n",
      "richard meixner has re...\n",
      "\n",
      "hawkeye entertainment <sbiz> completes offering - 26-mar-1987 17:01:22.82\n",
      "hawkeye entertainment inc said it\n",
      "completed its in...\n",
      "\n",
      "chadian troops reportedly recapture faya - 27-mar-1987 13:14:19.81\u0005\u0005\u0005v rm y\n",
      "chadian troops have recaptured the\n",
      "northern oasis ...\n",
      "\n",
      "musikahn corp files for reorganization - 12-mar-1987 16:03:01.84\n",
      "<musikahn corp> said it filed a\n",
      "reorganization pla...\n",
      "\n",
      "sony plans video and tape production in thailand - 17-apr-1987 11:47:35.61\n",
      "<sony corp> of japan plans to launch a\n",
      "15-mln-dlr ...\n",
      "\n",
      "viacom's <via> mtv to air in australia - 17-mar-1987 13:59:26.98\n",
      "viacom international inc's\n",
      "mtv networks entertainm...\n",
      "\n",
      "kurzweil <kurm> has reduced work force - 11-mar-1987 14:57:17.70\n",
      "kurzweil music systems inc said\n",
      "it has taken steps...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search(query_string, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f236c9-0f67-4864-80a0-e1fbe977b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4e85e-ecc7-42a3-a761-5016b8bd010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"app\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/May/2022 23:26:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2022 23:26:06] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2022 23:26:06] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2022 23:26:06] \"GET /static/foogle.png HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "server = flask.Flask('app')\n",
    "\n",
    "app = dash.Dash('app', server = server)\n",
    "app.css.config.serve_locally = True\n",
    "app.scripts.config.serve_locally = True\n",
    "\n",
    "app.layout = html.Main([\n",
    "     html.Link(\n",
    "        rel='stylesheet',\n",
    "        href='/static/index.css'\n",
    "    ),\n",
    "    html.Center([\n",
    "        html.Img(src=\"/static/foogle.png\", width=\"20%\", height=\"5%\", id=\"googleimg\"),\n",
    "        html.Div([\n",
    "            html.Div([\n",
    "                html.Span([\n",
    "                    dcc.Input(name=\"search\", type=\"text\", placeholder=\"O que gostava de saber?\"),\n",
    "                ], id=\"inputspan\"),\n",
    "            ], id=\"maindiv\")\n",
    "        ]),\n",
    "        html.Section([\n",
    "            html.Div([\n",
    "                html.Button('Pesquisa Foogle', id='search-valu<e', n_clicks=0),\n",
    "            ]),\n",
    "            html.Div([\n",
    "                html.Button('Estou com sorte', id='lucky-value', n_clicks=0),\n",
    "            ]),\n",
    "        ]),\n",
    "    ]),   \n",
    "])\n",
    "\n",
    "@app.server.route('/static/<path:path>')\n",
    "def static_file(path):\n",
    "    static_folder = os.path.join(os.getcwd(), 'static')\n",
    "    return send_from_directory(static_folder, path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49500395-3c07-40be-8e29-7942c3841c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd2af3-19b6-43c5-8481-9f2c059ca89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
